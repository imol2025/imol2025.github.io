---
layout: page
title: "Anna Jordanous"
---

<div style="text-align:center; margin-bottom:20px;">
  <img src="/assets/img/AnnaJordanous.jpg" alt="Anna Jordanous" style="max-width:220px; border-radius:8px;">
  <p><strong>University of Kent, United Kingdom</strong></p>
</div>

---

# TALK: *"Measuring success in open ended-learning AI models: what has been learned from computational creativity evaluation research"*

**Abstract**  
Evaluation of success is vital for scientific progress. Success in open ended learning is difficult to specify or measure in an interpretable or psychologically valid way, and progress is driven by goals other than achieving benchmarks or intended targets. There is quite a natural comparison to how evaluating creativity presents the same challenges. I will discuss how computational creativity research handles evaluation of creative software, including the practical challenges that are not faced in other types of AI evaluation. While Computational Creativity and creative AI researchers have had some success in agreeing evaluation approaches, there are also many unsolved issues that remain unsolved. Many past methods fail to scale up to current generative AI. Yet all is not lost; I will finish by reflecting on what I find exciting for the future in such evaluation.
