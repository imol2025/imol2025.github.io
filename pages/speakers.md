---
layout: page
title: Speakers
subtitle: 
---




<div class='row'>
  <div class="col-3">
    <div class="frame">
      <img class="speaker-img" src='/assets/img/romy.jpeg'>
    </div>
  </div>

  <div class="col-9">
    <h4> <a href="https://research.birmingham.ac.uk/en/persons/romy-froemer" target="_blank"> Romy Frömer </a></h4>
    <p class='speaker-affiliation'> University of Birmingham </p>
      <p style='font-size: 11pt;'>
        <b>Talk title: </b>  Hidden knobs: Mechanisms of flexible, goal-directed decision-making
    </p>
    <button type="button" style='font-size: 11pt;' class="collapsible">Bio</button>
    <div class="content">
        <p style='margin-top: 5pt;font-size: 11pt;'>
            Dr Froemer completed her undergraduate and graduate degrees at Humboldt-Universität zu Berlin. Following a one-year postdoc with Professor Abdel Rahman at Humboldt-Universität zu Berlin, she worked with Amitai Shenhav at Brown University as a postdoctoral fellow and subsequently senior research associate. She joined University of Birmingham as an Assistant Professor in January 2023.    
        </p>    
    </div>
    <button type="button" style='font-size: 11pt;' class="collapsible">Abstract</button>
    <div class="content">
        <p style='margin-top: 5pt;font-size: 11pt;'>
            Sequential sampling models have been tremendously successful in describing choice behavior and generating testable neural predictions. Missing to date is how these same mechanisms can flexibly give rise to the broad range of decisions humans make every day, including choosing the item they like most or least, or assigning a value to their option set as a whole. To test whether and how a single sequential sampling model could flexibly accommodate these and other types of decisions, we developed a theoretical framework that formalizes the necessary representations that align sequential sampling and evidence accumulation with one's current choice goals. We implement this framework within an extended leaky competing accumulator model and show that model simulations can parsimoniously explain behavior across a range of different choice goals, while also generating predictions for previously untested choice goals. Testing novel behavioral predictions of our model, we show that human behavior matches the predicted patterns, and further expand on previous work demonstrating that value-related patterns in gaze critically depend on the goals people have when engaging with options at hand.
        </p>
    </div>
  </div>
</div>

<div class='row'>
  <div class="col-3">
    <div class="frame">
      <img class="speaker-img" src='/assets/img/ted.png'>
    </div>
  </div>

  <div class="col-9">
    <h4> <a href="https://en.wikipedia.org/wiki/Ted_Chiang" target="_blank"> Ted Chiang </a></h4>
    <p class='speaker-affiliation'> Writer, Santa Fe Institute </p>
      <p style='font-size: 11pt;'>
        <b>Talk title: </b> Speculations on educating artificial lifeforms
    </p>
    <button type="button" style='font-size: 11pt;' class="collapsible">Bio</button>
    <div class="content">
        <p style='margin-top: 5pt;font-size: 11pt;'>
Ted Chiang's fiction has won four Hugo, four Nebula, and six Locus Awards, and has been reprinted in Best American Short Stories. His first collection Stories of Your Life and 
Others has been translated into twenty-one languages, and the title story was the basis for the Oscar-nominated film Arrival. His second collection Exhalation was chosen by The 
New York Times as one of the 10 Best Books of 2019. (Photo vy Alan Berner)       </p>    
    </div>
    <button type="button" style='font-size: 11pt;' class="collapsible">Abstract</button>
    <div class="content">
        <p style='margin-top: 5pt;font-size: 11pt;'>
If we ever create artificial lifeforms capable of human-like cognition, how might we teach them the things we want them to know? The author of "The Lifecycle of Software Objects" describes some of the thinking that went into the scenario depicted in the novella. Some of the topics mentioned include the relationship between artificial intelligence and artificial life, Piaget's stages of cognitive development, the differences between speech and writing, and the ethics of owning intelligent beings.        </p>
    </div>
  </div>
</div>

<div class='row'>
  <div class="col-3">
    <div class="frame">
      <img class="speaker-img" src='/assets/img/alison.jpeg'>
    </div>
  </div>

  <div class="col-9">
    <h4> <a href="http://alisongopnik.com/" target="_blank"> Alison Gopnik </a></h4>
    <p class='speaker-affiliation'> UC Berkley </p>
      <p style='font-size: 11pt;'>
        <b>Talk title: </b> Empowerment as causal learning, causal learning as empowerment: A bridge between Bayesian causal hypothesis testing and reinforcement learning
    </p>
    <button type="button" style='font-size: 11pt;' class="collapsible">Bio</button>
    <div class="content">
        <p style='margin-top: 5pt;font-size: 11pt;'>
            Alison Gopnik is a professor of psychology and affiliate professor of philosophy at the University of California at Berkeley, and a member of the Berkeley AI Research Group. She received her BA from McGill University and her PhD. from Oxford University. She is a leader in cognitive science, particularly the study of children’s learning and development. She was one of the founders of the field of “theory of mind”, an originator of the “theory theory” of cognitive development, and the first to apply Bayesian probabilistic models to children’s learning. She has received the APS Lifetime Achievement Cattell and William James Awards, the Bradford Washburn Award for Science Communication, the SRCD Lifetime Achievement Award for Basic Science in Child Development and the Rumelhart Prize for Theoretical Foundations of Cognitive Science. She is an elected member of the Society of Experimental Psychologists and the American Academy of Arts and Sciences and a Cognitive Science Society, American Association for the Advancement of Science, and Guggenheim Fellow. She was 2022-23 President of the Association for Psychological Science.
            She is the author or coauthor of over 150 journal articles and several books including “Words, thoughts and theories” MIT Press, 1997, and the bestselling and critically acclaimed popular books “The Scientist in the Crib” William Morrow, 1999, “The Philosophical Baby; What children’s minds tell us about love, truth and the meaning of life” 2009, and “The Gardener and the Carpenter” 2016, Farrar, Strauss and Giroux, the latter two won the Cognitive Development Society Best Book Prize in 2009 and 2016. She has also written widely about cognitive science and psychology for The Wall Street Journal, The New York Times, The Economist, The Atlantic, The New Yorker, Scientific American, The Times Literary Supplement, The New York Review of Books, New Scientist and Slate, among others. Her TED talk on her work has been viewed more than 5.5 million times. She has frequently appeared on TV, radio and podcasts including “The Charlie Rose Show”, “The Colbert Report”, and “The Ezra Klein Show”. Photo by Rod Searcey.
        </p>    
    </div>
    <button type="button" style='font-size: 11pt;' class="collapsible">Abstract</button>
    <div class="content">
        <p style='margin-top: 5pt;font-size: 11pt;'>
            Learning about the causal structure of the world is a fundamental problem for human cognition, and causal knowledge is central to both intuitive and scientific theories. Cognitive scientists have applied advances in our formal understanding of causation in philosophy and computer science, particularly within the Causal Bayes Net formalism, to understand human causal learning. In parallel, in the very different tradition of reinforcement learning, researchers have developed the idea of an intrinsic reward signal called “empowerment”. An agent is rewarded for maximizing the mutual information between its actions and their outcomes, regardless of the external reward value of those outcomes. In other words, the agent is rewarded if variation in an action systematically leads to parallel variation in an outcome so that variation in the action predicts variation in the outcome. The result is an agent that has maximal control over its environment. This is very close to the conception of causal knowledge and learning in the prevailing “interventionist” accounts of causal knowledge that underpin the Bayes net formalism. I argue that “empowerment” may be an important bridge between classical Bayesian causal learning and reinforcement learning and may help to characterize causal learning in humans and enable it in machines. Empowerment may also explain distinctive empirical features of children’s causal learning, as well as providing a more tractable computational account of how that learning is possible. Finally I will present some empirical results exploring empowerment in children and adults.
        </p>
    </div>
  </div>
</div>


<div class='row'>
  <div class="col-3">
    <div class="frame">
      <img class="speaker-img" src='/assets/img/kou.jpg'>
    </div>
  </div>

  <div class="col-9">
    <h4> <a href="https://motivationsciencelab.com/principal-investigator/" target="_blank"> Kou Murayama </a></h4>
    <p class='speaker-affiliation'> University of Tübingen </p>
      <p style='font-size: 11pt;'>
        <b>Talk title: </b> Knowledge as a sustainable source of internal rewards: How curiosity transforms into enduring interest 
    </p>
    <button type="button" style='font-size: 11pt;' class="collapsible">Bio</button>
    <div class="content">
        <p style='margin-top: 5pt;font-size: 11pt;'>
            Kou Murayama is the PI of the Motivation Science Lab. He received a Ph. D. in Educational Psychology at the University of Tokyo in 2006. After doing research in several different institutions in Japan, the United States, and Germany in a variety of disciplines (Tokyo Institute of Technology, University of Rochester, University of Munich, University of California, Los Angeles), he became a Lecturer at the University of Reading, UK in 2013. In 2020, Kou Murayama started his current role as a full Professor at the University of Tübingen. Kou Murayama is a recipient of various awards such as F. J. McGuigan Early Career Investigator Prize (American Psychological Foundation), Leverhulme Trust Research Leadership Award (Leverhulme Trust), Advanced Research Fellowships (Jacobs Foundation), and the Humboldt Professorship (Alexander von Humboldt Foundation). His work focuses on a number of overlapping questions about how motivation works in human functioning.
        </p>    
    </div>
    <button type="button" style='font-size: 11pt;' class="collapsible">Abstract</button>
    <div class="content">
        <p style='margin-top: 5pt;font-size: 11pt;'>
            In recent years, interdisciplinary research on curiosity has surged, exploring how individuals naturally pursue knowledge without external incentives. However, this growing field has largely overlooked the extensive psychological research on interest development and trait curiosity/interest. This talk presents a unifying conceptual perspective, the reward-learning framework of knowledge acquisition (Murayama, 2022), which bridges these previously separate areas. The framework posits that knowledge acquisition acts as an intrinsic reward, reinforcing information-seeking behaviors through a reward-learning process. Importantly, this process can establish a positive feedback loop that sustains and amplifies information-seeking over time. This perspective offers a compelling explanation for how momentary curiosity can evolve into a long-term inclination for knowledge (trait curiosity/interest). This perspective also underscores the need to integrate semantic sense-making processes into decision-making models. 
        </p>
    </div>
  </div>
</div>

<div class='row'>
  <div class="col-3">
    <div class="frame">
      <img class="speaker-img" src='/assets/img/pierluca.jpg'>
    </div>
  </div>

  <div class="col-9">
    <h4> <a href="https://proceduralia.github.io/" target="_blank"> Pierluca D'Oro </a></h4>
    <p class='speaker-affiliation'> McGill University, Meta </p>
      <p style='font-size: 11pt;'>
        <b>Talk title: </b> AI-assisted agent design with large language models
    </p>
    <button type="button" style='font-size: 11pt;' class="collapsible">Bio</button>
    <div class="content">
        <p style='margin-top: 5pt;font-size: 11pt;'>
            Pierluca D'Oro is a Sicilian scientist working on building agents with reinforcement learning. He studied in Italy, at the University of Catania and Politecnico di Milano, and he is currently finishing his PhD at Mila and Université de Montréal. His primary research focus is to make reinforcement learning algorithms more intuitive for an agent designer, by increasing the synergy between neural networks and the reinforcement learning machinery, and by leveraging the help of AI assistants.
        </p>    
    </div>
    <button type="button" style='font-size: 11pt;' class="collapsible">Abstract</button>
    <div class="content">
        <p style='margin-top: 5pt;font-size: 11pt;'>
            The job of an agent designer is hard -- can AI assistants make it easier? In this talk, I discuss the paradigm of AI-assisted Agent Design, where an AI assistant collaborates with an agent designer to create more capable agents. As a case study, I will talk about training agents to play the game of NetHack. First, I will show how a reward function distilled from a LLM-based assistant can intrinsically motivate an agent, leading it to play the game even better than a hand-crafted reward. Then, I will introduce the paradigm of AI-assisted Skill Design, which allows human designers and AI assistants to co-create a versatile library of agent skills. I will then show a method for AI-assisted Skill Design, that can solve tasks specified in language without any task-specific training.
        </p>
    </div>
  </div>
</div>

<div class='row'>
  <div class="col-3">
    <div class="frame">
      <img class="speaker-img" src='/assets/img/anna.jpg'>
    </div>
  </div>

  <div class="col-9">
    <h4> <a href="https://anna.harutyunyan.net/" target="_blank"> Anna Harutyunyan </a></h4>
    <p class='speaker-affiliation'> Google DeepMind </p>
      <p style='font-size: 11pt;'>
        <b>Talk title: </b> Art of decision, science of nondoing
    </p>
    <button type="button" style='font-size: 11pt;' class="collapsible">Bio</button>
    <div class="content">
        <p style='margin-top: 5pt;font-size: 11pt;'>
            Anna was born and raised in Armenia. Her winding academic path has led her through post-Soviet mathematical landscapes and Oregonian planar graphs, to eventually and almost accidentally, reinforcement learning in 2013. She received her PhD in 2018 from the Free University of Brussels, with a thesis titled "Beyond single step temporal difference learning". Since then she has been a research scientist at DeepMind, continuing to ask questions beyond the convention. A student of wisdom traditions, she is now interested in philosophically and metaphysically motivated technology, poetics of agents, and seeks to align the computational metaphors underlying our algorithms with deeper truths about human and more-than-human intelligence. 
        </p>    
    </div>
    <button type="button" style='font-size: 11pt;' class="collapsible">Abstract</button>
    <div class="content">
        <p style='margin-top: 5pt;font-size: 11pt;'>
            TBA
        </p>
    </div>
  </div>
</div>

<div class='row'>
  <div class="col-3">
    <div class="frame">
      <img class="speaker-img" src='/assets/img/brian.jpg'>
    </div>
  </div>

  <div class="col-9">
    <h4> <a href="https://brianchristian.org/" target="_blank"> Brian Christian </a></h4>
    <p class='speaker-affiliation'> Writer - Univ. of Oxford </p>
      <p style='font-size: 11pt;'>
        <b>Panel Moderator</b>
    </p>
    <button type="button" style='font-size: 11pt;' class="collapsible">Bio</button>
    <div class="content">
        <p style='margin-top: 5pt;font-size: 11pt;'>
            Brian Christian is a DPhil researcher working on intrinsic motivation, alignment, and value representation in the Human Information Processing Lab at the University of Oxford, as well as an affiliate of UC Berkeley’s Center for Human-Compatible AI, UC Berkeley’s Center for IT Research in the Interest of Society, and the Institute for Advanced Study’s AI Policy and Governance Working Group. He is best known as the author of a series of acclaimed nonfiction books about the human implications of computer science, including The Most Human Human, Algorithms to Live By, and The Alignment Problem, which The New York Times ranked as the #1 nonfiction book on AI in 2024.
        </p>    
    </div>
  </div>
</div>